# Veo on Vertex AI video generation API  |  Generative AI on Vertex AI  |  Google Cloud Documentation
[Skip to main content](#main-content)

*   [Technology areas](https://docs.cloud.google.com/docs)
    *   [Guides](https://docs.cloud.google.com/vertex-ai/generative-ai/docs)
    *   [API reference](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest)
    *   [Vertex AI Cookbook](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/cookbook)
    *   [Prompt gallery](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/prompt-gallery)
    *   [Resources](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/getting-help)
    *   [FAQ](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/faq)
    *   [Pricing](https://cloud.google.com/vertex-ai/generative-ai/pricing)
*   [Cross-product tools](https://docs.cloud.google.com/docs/cross-product-overviews)
*   [Console](https://console.cloud.google.com/)

*   REST and RPC reference
    
*   Vertex AI in express mode
    
    *   [REST API reference](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/vertex-ai-express-mode-api-reference)
    *   v1
        
        *   [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/express-mode/rest/v1/publishers.models)
        *   [countTokens](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/express-mode/rest/v1/publishers.models/countTokens)
        *   [generateContent](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/express-mode/rest/v1/publishers.models/generateContent)
        *   [streamGenerateContent](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/express-mode/rest/v1/publishers.models/streamGenerateContent)
        
    
*   REST
    
    *   [All methods](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest)
    *   v1
        
        *   REST Resources
            
        
        *   projects.locations.reasoningEngines.sessions.events
            
            *   [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/projects.locations.reasoningEngines.sessions.events)
            *   [list](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/projects.locations.reasoningEngines.sessions.events/list)
            
        
        *   Types
            
        *   [AgentConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/AgentConfig)
        *   [AggregationMetric](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/AggregationMetric)
        *   [ApiAuth](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/ApiAuth)
        *   [ApiKeyConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/ApiKeyConfig)
        *   [AutoraterConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/AutoraterConfig)
        *   [BigQueryDestination](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/BigQueryDestination)
        *   [BigQuerySource](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/BigQuerySource)
        *   [ComputeTokensResponse](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/ComputeTokensResponse)
        *   [Content](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/Content)
        *   [CountTokensResponse](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/CountTokensResponse)
        *   [EncryptionSpec](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/EncryptionSpec)
        *   [EnvVar](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/EnvVar)
        *   [Fact](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/Fact)
        *   [GcsDestination](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/GcsDestination)
        *   [GcsSource](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/GcsSource)
        *   [GenerateContentResponse](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/GenerateContentResponse)
        *   [GenerationConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/GenerationConfig)
        *   [GroundingMetadata](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/GroundingMetadata)
        *   [HarmBlockThreshold](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/HarmBlockThreshold)
        *   [HarmCategory](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/HarmCategory)
        *   [InstanceData](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/InstanceData)
        *   [JobState](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/JobState)
        *   [ManagedTopicEnum](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/ManagedTopicEnum)
        *   [MemoryTopicId](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/MemoryTopicId)
        *   [Metric](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/Metric)
        *   [ModalityTokenCount](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/ModalityTokenCount)
        *   [ModelArmorConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/ModelArmorConfig)
        *   [PredictResponse](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/PredictResponse)
        *   [RagChunk](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/RagChunk)
        *   [RagEngineConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/RagEngineConfig)
        *   [RagFileTransformationConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/RagFileTransformationConfig)
        *   [Rubric](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/Rubric)
        *   [SafetySetting](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/SafetySetting)
        *   [Schema](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/Schema)
        *   [SessionEvent](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/SessionEvent)
        *   [StreamingPredictResponse](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/StreamingPredictResponse)
        *   [Tensor](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1/Tensor)
        
    *   v1beta1
        
        *   REST Resources
            
        
        *   projects.locations.reasoningEngines.sessions.events
            
            *   [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/projects.locations.reasoningEngines.sessions.events)
            *   [list](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/projects.locations.reasoningEngines.sessions.events/list)
            
        
        *   Types
            
        *   [AgentConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/AgentConfig)
        *   [AggregationMetric](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/AggregationMetric)
        *   [ApiAuth](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/ApiAuth)
        *   [ApiKeyConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/ApiKeyConfig)
        *   [AuthConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/AuthConfig)
        *   [AutoraterConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/AutoraterConfig)
        *   [BigQueryDestination](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/BigQueryDestination)
        *   [BigQuerySource](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/BigQuerySource)
        *   [CacheConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/CacheConfig)
        *   [ComputeTokensResponse](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/ComputeTokensResponse)
        *   [Content](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/Content)
        *   [CountTokensResponse](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/CountTokensResponse)
        *   [EncryptionSpec](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/EncryptionSpec)
        *   [EnvVar](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/EnvVar)
        *   [Fact](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/Fact)
        *   [FunctionDeclaration](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/FunctionDeclaration)
        *   [GcsDestination](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/GcsDestination)
        *   [GcsSource](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/GcsSource)
        *   [GenerateContentResponse](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/GenerateContentResponse)
        *   [GenerationConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/GenerationConfig)
        *   [GroundingMetadata](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/GroundingMetadata)
        *   [HarmBlockThreshold](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/HarmBlockThreshold)
        *   [HarmCategory](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/HarmCategory)
        *   [InstanceData](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/InstanceData)
        *   [JobState](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/JobState)
        *   [ManagedTopicEnum](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/ManagedTopicEnum)
        *   [MemoryTopicId](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/MemoryTopicId)
        *   [Metric](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/Metric)
        *   [ModalityTokenCount](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/ModalityTokenCount)
        *   [ModelArmorConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/ModelArmorConfig)
        *   [PredictResponse](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/PredictResponse)
        *   [RagChunk](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/RagChunk)
        *   [RagEngineConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/RagEngineConfig)
        *   [RagFileChunkingConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/RagFileChunkingConfig)
        *   [RagFileMetadataConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/RagFileMetadataConfig)
        *   [RagFileParsingConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/RagFileParsingConfig)
        *   [RagFileTransformationConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/RagFileTransformationConfig)
        *   [Rubric](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/Rubric)
        *   [SafetySetting](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/SafetySetting)
        *   [Schema](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/Schema)
        *   [SessionEvent](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/SessionEvent)
        *   [StreamingPredictResponse](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/StreamingPredictResponse)
        *   [Tensor](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/Tensor)
        *   [Tool](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/v1beta1/Tool)
        
    *   Shared types
        
        *   Types
            
        *   [AlternateInitConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/AlternateInitConfig)
        *   [AuditConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/AuditConfig)
        *   [BackgroundSwapProcessingConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/BackgroundSwapProcessingConfig)
        *   [Binding](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/Binding)
        *   [BoundingPoly](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/BoundingPoly)
        *   [CacheConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/CacheConfig)
        *   [CancelOperationRequest](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/CancelOperationRequest)
        *   [ControlNetConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/ControlNetConfig)
        *   [ControlType](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/ControlType)
        *   [Date](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/Date)
        *   [DeleteOperationRequest](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/DeleteOperationRequest)
        *   [EditConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/EditConfig)
        *   [EditConfigV6](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/EditConfigV6)
        *   [EditMode](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/EditMode)
        *   [GenSelfieConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/GenSelfieConfig)
        *   [GetIamPolicyRequest](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/GetIamPolicyRequest)
        *   [GetOperationRequest](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/GetOperationRequest)
        *   [HttpBody](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/HttpBody)
        *   [ImageOutputOptions](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/ImageOutputOptions)
        *   [LatLng](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/LatLng)
        *   [ListOperationsRequest](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/ListOperationsRequest)
        *   [ListOperationsResponse](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/ListOperationsResponse)
        *   [LogType](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/LogType)
        *   [MaskMode](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/MaskMode)
        *   [NamedBoundingBox](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/NamedBoundingBox)
        *   [OutpaintingProcessingConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/OutpaintingProcessingConfig)
        *   [OutputOptions](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/OutputOptions)
        *   [Policy](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/Policy)
        *   [ReferenceType](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/ReferenceType)
        *   [SafetyAttributes](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/SafetyAttributes)
        *   [SemanticFilterConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/SemanticFilterConfig)
        *   [SemanticFilterResponse](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/SemanticFilterResponse)
        *   [SetIamPolicyRequest](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/SetIamPolicyRequest)
        *   [SubjectType](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/SubjectType)
        *   [TaskType](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/TaskType)
        *   [TestIamPermissionsRequest](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/TestIamPermissionsRequest)
        *   [TestIamPermissionsResponse](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/TestIamPermissionsResponse)
        *   [TextEmbedding](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/TextEmbedding)
        *   [TextEmbeddingPredictionInstance](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/TextEmbeddingPredictionInstance)
        *   [TextEmbeddingPredictionParams](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/TextEmbeddingPredictionParams)
        *   [TextEmbeddingPredictionResult](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/TextEmbeddingPredictionResult)
        *   [UpscaleConfig](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/UpscaleConfig)
        *   [VideoGenerationModelInstance](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/VideoGenerationModelInstance)
        *   [VideoGenerationModelParams](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/VideoGenerationModelParams)
        *   [VideoGenerationModelResult](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/VideoGenerationModelResult)
        *   [VirtualTryOnModelInstance](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/VirtualTryOnModelInstance)
        *   [VirtualTryOnModelParams](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/VirtualTryOnModelParams)
        *   [VirtualTryOnModelResultProto](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/VirtualTryOnModelResultProto)
        *   [VisionEmbeddingModelInstance](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/VisionEmbeddingModelInstance)
        *   [VisionEmbeddingModelParams](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/VisionEmbeddingModelParams)
        *   [VisionEmbeddingModelResult](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/VisionEmbeddingModelResult)
        *   [VisionGenerativeModelInstance](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/VisionGenerativeModelInstance)
        *   [VisionGenerativeModelParams](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/VisionGenerativeModelParams)
        *   [VisionGenerativeModelResult](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/VisionGenerativeModelResult)
        *   [VisionReasoningModelInstance](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/VisionReasoningModelInstance)
        *   [VisionReasoningModelParams](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/VisionReasoningModelParams)
        *   [VisionReasoningModelResult](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/VisionReasoningModelResult)
        *   [WaitOperationRequest](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rest/Shared.Types/WaitOperationRequest)
        
    
*   RPC
    
    *   [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rpc)
    *   [cloud.ai.platform.common](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rpc/cloud.ai.platform.common)
    *   [google.api](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rpc/google.api)
    
    *   google.cloud.aiplatform.v1beta1
        
        *   [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rpc/google.cloud.aiplatform.v1beta1)
        
    *   [google.iam.v1](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rpc/google.iam.v1)
    *   [google.longrunning](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rpc/google.longrunning)
    *   [google.rpc](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rpc/google.rpc)
    *   [google.type](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/rpc/google.type)
    
*   Capabilities
    
    *   Imagen API
        
        *   [Generate images](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/imagen-api)
        *   [Edit images](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/imagen-api-edit)
        *   [Customize images (few-shot)](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/imagen-api-customization)
        *   [Virtual Try-On API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/virtual-try-on-api)
        *   [Product Recontext API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/imagen-product-recontext-api)
        *   [Upscale API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/imagen-upscale-api)
        
    *   [Veo video generation API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/veo-video-generation)
    *   [Lyria music generation API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/lyria-music-generation)
    *   [Batch prediction API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api)
    *   [Tuning API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/tuning)
    *   [Gen AI Evaluation API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/evaluation)
    
    *   [CountTokens API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/count-tokens)
    *   [MedLM API](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/medlm)
    
*   Google Gen AI SDK
    
*   [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/sdks/overview)
*   [Python](https://googleapis.github.io/python-genai/)
*   [Go](https://pkg.go.dev/google.golang.org/genai)
*   [Java](https://github.com/googleapis/java-genai)
*   [Node.js](https://github.com/googleapis/nodejs-genai)
*   Vertex AI SDK
    
*   [Overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/libraries)
*   [Python](https://docs.cloud.google.com/python/docs/reference/vertexai/latest)
*   [Node.js](https://docs.cloud.google.com/nodejs/docs/reference/vertexai/latest)
*   [Java](https://docs.cloud.google.com/java/docs/reference/google-cloud-vertexai/latest)
*   [Go](https://docs.cloud.google.com/go/docs/reference/cloud.google.com/go/vertexai/latest)
*   [C#](https://docs.cloud.google.com/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest)
*   Agent Development Kit (ADK)
    
*   [Overview](https://google.github.io/adk-docs/api-reference/)

Veo on Vertex AI video generation API
-------------------------------------

Veo is the name of the model that supports video generation. Veo generates a video from a text prompt or an image prompt that you provide. For more information about Veo, see [Veo video generation overview](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/video/overview).

To explore this model in the console, see the `Video Generation` model card in the Model Garden.

[Try Veo on Vertex AI (Vertex AI Studio)](https://console.cloud.google.com/vertex-ai/studio/media)

[Try Veo in a Colab](https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/veo3_video_generation.ipynb)

Supported Models
----------------

Veo API supports the following models:

*   [`veo-2.0-generate-001`](about:/vertex-ai/generative-ai/docs/models/veo/2-0-generate#2.0-generate-001)
*   [`veo-2.0-generate-exp`](about:/vertex-ai/generative-ai/docs/models/veo/2-0-generate#2.0-generate-exp)
*   [`veo-2.0-generate-preview`](about:/vertex-ai/generative-ai/docs/models/veo/2-0-generate#2.0-generate-preview)
*   [`veo-3.0-generate-001`](about:/vertex-ai/generative-ai/docs/models/veo/3-0-generate#3.0-generate-001)
*   [`veo-3.0-fast-generate-001`](about:/vertex-ai/generative-ai/docs/models/veo/3-0-generate#3.0-fast-generate-001)
*   [`veo-3.0-generate-preview`](about:/vertex-ai/generative-ai/docs/models/veo/3-0-generate#3.0-generate-preview) (Preview)
*   [`veo-3.0-fast-generate-preview`](about:/vertex-ai/generative-ai/docs/models/veo/3-0-generate#3.0-fast-generate-preview) (Preview)
*   [`veo-3.1-generate-001`](about:/vertex-ai/generative-ai/docs/models/veo/3-1-generate#3.1-generate-001)
*   [`veo-3.1-fast-generate-001`](about:/vertex-ai/generative-ai/docs/models/veo/3-1-generate#3.1-fast-generate-001)
*   [`veo-3.1-generate-preview`](about:/vertex-ai/generative-ai/docs/models/veo/3-1-generate#3.1-generate-preview) (Preview)
*   [`veo-3.1-fast-generate-preview`](about:/vertex-ai/generative-ai/docs/models/veo/3-1-generate#3.1-fast-generate-preview) (Preview)

For more information, see [Veo models](about:/vertex-ai/generative-ai/docs/models#veo-models).

HTTP request
------------

```
curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID:predictLongRunning \

-d '{
  "instances": [
    {
      "prompt": string,
      "image": {
        // Union field can be only one of the following:
        "bytesBase64Encoded": string,
        "gcsUri": string,
        // End of list of possible types for union field.
        "mimeType": string
      },
      "lastFrame": {
        // Union field can be only one of the following:
        "bytesBase64Encoded": string,
        "gcsUri": string,
        // End of list of possible types for union field.
        "mimeType": string
      },
      "video": {
        // Union field can be only one of the following:
        "bytesBase64Encoded": string,
        "gcsUri": string,
        // End of list of possible types for union field.
        "mimeType": string
      },
      "mask": {
        // Union field can be only one of the following:
        "bytesBase64Encoded": string,
        "gcsUri": string,
        // End of list of possible types for union field.
        "mimeType": string,
        "maskMode": string
      },
      "referenceImages": [
        // A list of up to three asset images or at most one style image for the
        // model to use when generating videos.
        //
        // referenceImages is supported by the following models:
        //
        // *   veo-2.0-generate-exp (Preview)
        // *   veo-3.1-generate-preview (Preview)
        // *   veo-3.1-fast-generate-preview (Preview)
        {
        "image:" {
          // Union field can be only one of the following:
          "bytesBase64Encoded": string,
          "gcsUri": string,
          // End of list of possible types for union field.
          "mimeType": string
        },
        "referenceType": string
        }
      ]
    }
  ],
  "parameters": {
    "aspectRatio": string,
    "compressionQuality": string,
    "durationSeconds": integer,
    "enhancePrompt": boolean, // Veo 2 models only
    "generateAudio": boolean,
    "negativePrompt": string,
    "personGeneration": string,
    "resizeMode": string, // Veo 3 image-to-video only
    "resolution": string, // Veo 3 models only
    "sampleCount": integer,
    "seed": uint32,
    "storageUri": string
  }
}'

```


Instances
---------



* Instances:         prompt      
  *         string                  Required for text-to-video. Optional if an input image prompt is          provided (image-to-video).                          A text string to guide the first eight seconds in the video. For          example:                                      A fast-tracking shot through a bustling dystopian sprawl with            bright neon signs, flying cars and mist, night, lens flare,            volumetric lighting                                A neon hologram of a car driving at top speed, speed of light,            cinematic, incredible details, volumetric lighting                                Many spotted jellyfish pulsating under water. Their bodies are            transparent and glowing in deep ocean                                extreme close-up with a shallow depth of field of a puddle in a            street. reflecting a busy futuristic Tokyo city with bright neon            signs, night, lens flare                                Timelapse of the northern lights dancing across the Arctic sky,            stars twinkling, snow-covered landscape                                A lone cowboy rides his horse across an open plain at beautiful            sunset, soft light, warm colors                        
* Instances: image
  *         Union field                  Optional. An image to guide video generation, which can be either a bytesBase64Encoded          string that encodes an image or a gcsUri string URI to a          Cloud Storage bucket location.              
* Instances: lastFrame
  *         Union field                  Optional. An image of the last frame of a video to fill the space          between. lastFrame can be either a bytesBase64Encoded          string that encodes an image or a gcsUri string URI to a          Cloud Storage bucket location.                          lastFrame is supported by the following models:                          veo-2.0-generate-001 (Preview)          veo-3.0-generate-exp (Preview)          veo-3.1-generate-preview (Preview)          veo-3.1-fast-generate-preview (Preview)          veo-3.1-generate-001          veo-3.1-fast-generate-001              
* Instances: video
  *         Union field                  Optional. A Veo generated input video to extend in          length. The input video is subject to the following limitations:                          The input file must be MP4.          The length must be 1 to 30 seconds.          The frame rate must be 24 frames per second.          The resolution must be either 720p or 1080p.                          The output video is subject to the following limitations:                          The output file is MP4.          The extended length is 7 seconds.          The frame rate is 24 frames per second.          The resolution is 720p.                          You can provide either a bytesBase64Encoded          string that encodes a video or a gcsUri string URI to a          Cloud Storage bucket location.                          video is supported by the following models:                          veo-2.0-generate-001 (Preview)          veo-3.1-generate-preview (Preview)          veo-3.1-fast-generate-preview (Preview)              
* Instances: mask
  *         Union field                  Optional. An image of a mask to apply to a video to add or remove an          object from a video.          mask can be either a bytesBase64Encoded          string that encodes an image or a gcsUri string URI to a          Cloud Storage bucket location.                          mask is supported by          veo-2.0-generate-preview in Preview.              
* Instances: referenceImages
  *         list[referenceImages]                  Optional. A list of up to three asset images or at most one style          images that describes the referenceImages for          the model to use when generating videos.                                  referenceImages is supported by the following models:                              veo-2.0-generate-exp (Preview)            veo-3.1-generate-preview (Preview)            veo-3.1-fast-generate-preview (Preview)                
* Instances: referenceImages.image
  *         Union field                  Optional. Contains the reference images to use as subject matter          input. Each image can be either a bytesBase64Encoded          string that encodes an image or a gcsUri string URI to a          Cloud Storage bucket location.              
* Instances: referenceImages.referenceType
  *         string        Required in a referenceImages object. Specifies the type        of reference image provided. The following values are supported:                                      "asset": The reference image provides assets for the            generated video, such as: the scene, an object, or a character.                                            "style": The reference image provides style information            for the generated videos, such as: scene colors, lighting, or            texture.                                                
* Instances: bytesBase64Encoded
  *         string                  A bytes base64-encoded string of an image or video file. Used with          the following objects:                          image          video          lastFrame          referenceImages.image              
* Instances: gcsUri
  *         string        A string URI to a Cloud Storage bucket location. Used with          the following objects:                          image          video          lastFrame          referenceImages.image              
* Instances: mimeType
  *         string                  Required for the following objects:                          image          video          mask          lastFrame          referenceImages.image                Specifies the mime type of a video or image.        For images, the following mime types are accepted:                  image/jpeg          image/png          image/webp                For videos, the following mime types are accepted:                  video/mov          video/mpeg          video/mp4          video/mpg          video/avi          video/wmv          video/mpegps          video/flv              


Parameters
----------



* Parameters: aspectRatio
  *         string                  Optional. Specifies the aspect ratio of generated videos. The          following are accepted values:                          "16:9"          "9:16"                The default value is "16:9".      
* Parameters: compressionQuality
  *         string                  Optional. Specifies the compression quality of the generated videos.          The accepted values are "optimized" or          "lossless".                          The default is "optimized".              
* Parameters: durationSeconds
  *         integer                  Required. The length in seconds of video files that you want to          generate.                          The following are the accepted values:                          Veo 2 models:            5-8. The default is 8.                    Veo 3 models:            4,6, or 8. The default is            8.                                When using referenceImages:            8.                                    For more information, see Veo          models.              
* Parameters: enhancePrompt
  *          boolean                    Optional. Use Gemini to enhance your prompts. Accepted           values are true or false. The default value           is true.                             enhancePrompt is supported by the following models:                             veo-2.0-generate-001           veo-2.0-generate-preview           veo-2.0-generate-exp                
* Parameters: generateAudio
  *         boolean                  Required for Veo 3 models. Generate audio for the          video. Accepted values are true or false.                          generateAudio isn't supported by          veo-2.0-generate-001 or          veo-2.0-generate-exp.                          For more information about available Veo models, see Veo          models.              
* Parameters: negativePrompt
  *          string                    Optional. A text string that describes anything you want to           discourage the model from generating. For example:                             overhead lighting, bright colors           people, animals           multiple cars, wind               
* Parameters: personGeneration
  *         string                  Optional. The safety setting that controls whether people or face          generation is allowed. One of the following:                                      "allow_adult" (default value): allow generation of            adults only                                "dont_allow": disallows inclusion of people/faces in            images                                "allow_all": Allows the generation of people of all            ages. To use this value, your project must be on an allowlist.                        
* Parameters: resizeMode
  *         string                  Optional. Veo 3 models only, used with image for image-to-video. The          resize mode that the model uses to resize the video. Accepted values          are "pad" (default) or "crop".              
* Parameters: resolution
  *         string                  Optional. Veo 3 models only. The resolution of          the generated video. One of the following:                          "720p"          "1080p"          "4k" (Veo 3.1 Preview models only)                        The default value is "720p".              
* Parameters: sampleCount
  *         int                  Optional. The number of output videos requested. Accepted values are          1-4.              
* Parameters: seed
  *         uint32                  Optional. A number to request to make generated videos deterministic.          Adding a seed number with your request without changing other          parameters will cause the model to produce the same videos.                          The accepted range is 0-4,294,967,295.              
* Parameters: storageUri
  *         string                  Optional. A Cloud Storage bucket URI to store the output video, in          the format          gs://BUCKET_NAME/SUBDIRECTORY. If a          Cloud Storage bucket isn't provided, base64-encoded video          bytes are returned in the response.              


Sample requests
---------------

Use the following examples to create your own video request:

### Text-to-video generation request

### REST

To test a text prompt by using the Vertex AI Veo API, send a POST request to the publisher model endpoint.

Before using any of the request data, make the following replacements:

*   `PROJECT_ID`: A string representing your Google Cloud project ID.
*   `MODEL_ID`: A string respresenting the model ID to use. The following are accepted values:
    *   **Veo 2:** `"veo-2.0-generate-001"`
    *   **Veo 3:**`"veo-3.0-generate-001"`
    *   **Veo 3:**`"veo-3.0-fast-generate-001"`
    *   **Veo 3:**`"veo-3.0-generate-preview"` (Preview)
    *   **Veo 3:**`"veo-3.0-fast-generate-preview"` (Preview)
    *   **Veo 3.1:** `"veo-3.1-generate-001"`
    *   **Veo 3.1:** `"veo-3.1-fast-generate-001"`
*   `TEXT_PROMPT`: The text prompt used to guide video generation.
*   `OUTPUT_STORAGE_URI`: Optional: A string representing the Cloud Storage bucket to store the output videos. If not provided, video bytes are returned in the response. For example: `"gs://video-bucket/output/"`.
*   `RESPONSE_COUNT`: The number of video files to generate. The accepted range of values is `1`\-`4`.
*   `DURATION`: An integer representing the length of the generated video files. The following are the accepted values for each model:
    *   Veo 2 models: `5`\-`8`. The default is `8`.
    *   Veo 3 models: `4`, `6`, or `8`. The default is `8`.
*   **Additional optional parameters**
    
    Use the following optional variables depending on your use case. Add some or all of the following parameters in the `"parameters": {}` object.
    
    ```
"parameters": {
  "aspectRatio": "ASPECT_RATIO",
  "negativePrompt": "NEGATIVE_PROMPT",
  "personGeneration": "PERSON_SAFETY_SETTING",
  // "resolution": RESOLUTION, // Veo 3 models only
  "sampleCount": RESPONSE_COUNT,
  "seed": SEED_NUMBER
}
```

    
    *   `ASPECT_RATIO`: Optional: A string value that describes the aspect ratio of the generated videos. You can use the following values:
        
        *   `"16:9"` for landscape
        *   `"9:16"` for portrait
        
        The default value is `"16:9"`
        
    *   `NEGATIVE_PROMPT`: Optional: A string value that describes content that you want to prevent the model from generating.
    *   `PERSON_SAFETY_SETTING`: Optional: A string value that controls the safety setting for generating people or face generation. You can use the following values:
        
        *   `"allow_adult"`: Only allow generation of adult people and faces.
        *   `"disallow"`: Doesn't generate people or faces.
        
        The default value is `"allow_adult"`.
        
    *   `RESOLUTION`: Optional: A string value that controls the resolution of the generated video. Supported by **Veo 3 models only.** You can use the following values:
        
        *   `"720p"`
        *   `"1080p"`
        *   `"4k"` (Veo 3.1 Preview models only)
        
        The default value is `"720p"`.
        
    *   `RESPONSE_COUNT`: Optional. An integer value that describes the number of videos to generate. The accepted range of values is `1`\-`4`.
    *   `SEED_NUMBER`: Optional. An uint32 value that the model uses to generate deterministic videos. Specifying a seed number with your request without changing other parameters guides the model to produce the same videos. The accepted range of values is `0`\-`4294967295`.
    

HTTP method and URL:

```
POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning
```


Request JSON body:

```
{
  "instances": [
    {
      "prompt": "TEXT_PROMPT"
    }
  ],
  "parameters": {
    "storageUri": "OUTPUT_STORAGE_URI",
    "sampleCount": "RESPONSE_COUNT"
  }
}

```


To send your request, choose one of these options:

#### curl

Save the request body in a file named `request.json`, and execute the following command:

```
curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning"
```


#### PowerShell

Save the request body in a file named `request.json`, and execute the following command:

```
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
    -Method POST `
    -Headers $headers `
    -ContentType: "application/json; charset=utf-8" `
    -InFile request.json `
    -Uri "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning" | Select-Object -Expand Content
```


This request returns a full operation name with a unique operation ID. Use this full operation name to poll that status of the video generation request.

```
{
  "name": "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/a1b07c8e-7b5a-4aba-bb34-3e1ccb8afcc8"
}

```


### Image-to-video generation request

### REST

To test a text prompt by using the Vertex AI Veo API, send a POST request to the publisher model endpoint.

Before using any of the request data, make the following replacements:

*   `PROJECT_ID`: A string representing your Google Cloud project ID.
*   `MODEL_ID`: A string respresenting the model ID to use. The following are accepted values:
    *   **Veo 2:**`veo-2.0-generate-001`
    *   **Veo 3:**`veo-3.0-generate-001`
    *   **Veo 3.1:**`veo-3.1-generate-001`
    *   **Veo 3.1:**`veo-3.1-fast-generate-001`
*   `TEXT_PROMPT`: The text prompt used to guide video generation.
*   `INPUT_IMAGE`: A base64-encoded string that represents the input image. For best quality, we recommend that the input image's resolution be 720p (1280 x 720 pixels) or higher, and have an aspect ratio of either 16:9 or 9:16. Images of other aspect ratios or sizes may be resized or centrally cropped when the image is uploaded.
*   `MIME_TYPE`: A string representing the MIME type of the input image. Only the images of the following MIME types are supported:
    *   `"image/jpeg"`
    *   `"image/png"`
*   `OUTPUT_STORAGE_URI`: Optional: A string representing the Cloud Storage bucket to store the output videos. If not provided, video bytes are returned in the response. For example: `"gs://video-bucket/output/"`.
*   `RESIZE_MODE`: A string that represents the resize mode to use. The following are accepted values:
    *   `"crop"`: Crop the video to fit the new size.
    *   `"pad"`: Pad the video to fit the new size.
*   `RESPONSE_COUNT`: The number of video files to generate. The accepted range of values is `1`\-`4`.
*   `DURATION`: An integer representing the length of the generated video files. The following are the accepted values for each model:
    *   Veo 2 models: `5`\-`8`. The default is `8`.
    *   Veo 3 models: `4`, `6`, or `8`. The default is `8`.
*   **Additional optional parameters**
    
    Use the following optional variables depending on your use case. Add some or all of the following parameters in the `"parameters": {}` object.
    
    ```
"parameters": {
  "aspectRatio": "ASPECT_RATIO",
  "negativePrompt": "NEGATIVE_PROMPT",
  "personGeneration": "PERSON_SAFETY_SETTING",
  // "resolution": RESOLUTION, // Veo 3 models only
  "sampleCount": RESPONSE_COUNT,
  "seed": SEED_NUMBER
}
```

    
    *   `ASPECT_RATIO`: Optional: A string value that describes the aspect ratio of the generated videos. You can use the following values:
        
        *   `"16:9"` for landscape
        *   `"9:16"` for portrait
        
        The default value is `"16:9"`
        
    *   `NEGATIVE_PROMPT`: Optional: A string value that describes content that you want to prevent the model from generating.
    *   `PERSON_SAFETY_SETTING`: Optional: A string value that controls the safety setting for generating people or face generation. You can use the following values:
        
        *   `"allow_adult"`: Only allow generation of adult people and faces.
        *   `"disallow"`: Doesn't generate people or faces.
        
        The default value is `"allow_adult"`.
        
    *   `RESOLUTION`: Optional: A string value that controls the resolution of the generated video. Supported by **Veo 3 models only.** You can use the following values:
        
        *   `"720p"`
        *   `"1080p"`
        *   `"4k"` (Veo 3.1 Preview models only)
        
        The default value is `"720p"`.
        
    *   `RESPONSE_COUNT`: Optional. An integer value that describes the number of videos to generate. The accepted range of values is `1`\-`4`.
    *   `SEED_NUMBER`: Optional. An uint32 value that the model uses to generate deterministic videos. Specifying a seed number with your request without changing other parameters guides the model to produce the same videos. The accepted range of values is `0`\-`4294967295`.
    

HTTP method and URL:

```
POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning
```


Request JSON body:

```
{
  "instances": [
    {
      "prompt": "TEXT_PROMPT",
      "image": {
        "bytesBase64Encoded": "INPUT_IMAGE",
        "mimeType": "MIME_TYPE"
      }
    }
  ],
  "parameters": {
    "storageUri": "OUTPUT_STORAGE_URI",
    "sampleCount": RESPONSE_COUNT
    "resizeMode": "RESIZE_MODE"
  }
}

```


To send your request, choose one of these options:

#### curl

Save the request body in a file named `request.json`, and execute the following command:

```
curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning"
```


#### PowerShell

Save the request body in a file named `request.json`, and execute the following command:

```
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
    -Method POST `
    -Headers $headers `
    -ContentType: "application/json; charset=utf-8" `
    -InFile request.json `
    -Uri "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning" | Select-Object -Expand Content
```


This request returns a full operation name with a unique operation ID. Use this full operation name to poll that status of the video generation request.

```
{
  "name": "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/a1b07c8e-7b5a-4aba-bb34-3e1ccb8afcc8"
}

```


### Video request using asset images

### REST

To test a text prompt by using the Vertex AI Veo API, send a POST request to the publisher model endpoint.

Before using any of the request data, make the following replacements:

*   `PROJECT_ID`: Your Google Cloud project ID.
*   `MODEL_ID`: A string representing the model ID to use. The following are accepted values:
    *   **Veo 2: `veo-2.0-generate-exp`**
    *   **Veo 3: `veo-3.1-generate-preview`**
*   `TEXT_PROMPT`: The text prompt used to guide video generation.
*   `BASE64_ENCODED_IMAGE`: A base64-bytes encoded subject image. You can repeat this field and `mimeType` to specify up to three subject images.
*   `IMAGE_MIME_TYPE`: The MIME type of the input image. Only one of the following:
    
    *   `image/jpeg`
    *   `image/png`
    
    You can repeat this field and `bytesBase64Encoded` to specify up to three subject images.
    
*   `ASPECT_RATIO`: Optional: The aspect ratio of the generated video. Only one of the following:
    
    *   `"16:9"`
    *   `"9:16"`
    
    The default value is `"16:9"`.
    
*   `OUTPUT_STORAGE_URI`: Optional: The Cloud Storage bucket to store the output videos. If not provided, a Base64-bytes encoded video is returned in the response. For example: `gs://video-bucket/output/`.
*   `RESPONSE_COUNT`: The number of video files you want to generate. Accepted integer values: 1-4.
*   `VIDEO_RESOLUTION`: Optional: The resolution of the generated video. Only one of the following:
    
    *   `"720p"`
    *   `"1080p"`
    *   `"4k"` (Veo 3.1 Preview models only)
    
    Default value is `"720p"`.
    
*   **Additional optional parameters**
    
    Use the following optional variables depending on your use case. Add some or all of the following parameters in the `"parameters": {}` object.
    
    ```
"parameters": {
  "aspectRatio": "ASPECT_RATIO",
  "negativePrompt": "NEGATIVE_PROMPT",
  "personGeneration": "PERSON_SAFETY_SETTING",
  // "resolution": RESOLUTION, // Veo 3 models only
  "sampleCount": RESPONSE_COUNT,
  "seed": SEED_NUMBER
}
```

    
    *   `ASPECT_RATIO`: Optional: A string value that describes the aspect ratio of the generated videos. You can use the following values:
        
        *   `"16:9"` for landscape
        *   `"9:16"` for portrait
        
        The default value is `"16:9"`
        
    *   `NEGATIVE_PROMPT`: Optional: A string value that describes content that you want to prevent the model from generating.
    *   `PERSON_SAFETY_SETTING`: Optional: A string value that controls the safety setting for generating people or face generation. You can use the following values:
        
        *   `"allow_adult"`: Only allow generation of adult people and faces.
        *   `"disallow"`: Doesn't generate people or faces.
        
        The default value is `"allow_adult"`.
        
    *   `RESOLUTION`: Optional: A string value that controls the resolution of the generated video. Supported by **Veo 3 models only.** You can use the following values:
        
        *   `"720p"`
        *   `"1080p"`
        *   `"4k"` (Veo 3.1 Preview models only)
        
        The default value is `"720p"`.
        
    *   `RESPONSE_COUNT`: Optional. An integer value that describes the number of videos to generate. The accepted range of values is `1`\-`4`.
    *   `SEED_NUMBER`: Optional. An uint32 value that the model uses to generate deterministic videos. Specifying a seed number with your request without changing other parameters guides the model to produce the same videos. The accepted range of values is `0`\-`4294967295`.
    

HTTP method and URL:

```
POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning
```


Request JSON body:

```
{
  "instances": [
    {
      "prompt": "TEXT_PROMPT",
      // The following fields can be repeated for up to three total
      // images.
      "referenceImages": [
        {
          "image": {
            "bytesBase64Encoded": "BASE64_ENCODED_IMAGE",
            "mimeType": "IMAGE_MIME_TYPE"
          },
          "referenceType": "asset"
        }
      ]
    }
  ],
  "parameters": {
    "aspectRatio":, "ASPECT_RATIO,
    "durationSeconds": 8,
    "storageUri": "OUTPUT_STORAGE_URI",
    "sampleCount": RESPONSE_COUNT,
    "resolution": "VIDEO_RESOLUTION
  }
}

```


To send your request, choose one of these options:

#### curl

Save the request body in a file named `request.json`, and execute the following command:

```
curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning"
```


#### PowerShell

Save the request body in a file named `request.json`, and execute the following command:

```
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
    -Method POST `
    -Headers $headers `
    -ContentType: "application/json; charset=utf-8" `
    -InFile request.json `
    -Uri "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning" | Select-Object -Expand Content
```


This request returns a full operation name with a unique operation ID. Use this full operation name to poll that status of the video generation request.

```
{
  "name":
  "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/a1b07c8e-7b5a-4aba-bb34-3e1ccb8afcc8"
}

```


### Video request using a style image

### REST

To test a text prompt by using the Vertex AI Veo API, send a POST request to the publisher model endpoint.

Before using any of the request data, make the following replacements:

*   `PROJECT_ID`: Your Google Cloud project ID.
*   `MODEL_ID`: A string representing the model ID to use. Use the following value: `veo-2.0-generate-exp`.
    
*   `TEXT_PROMPT`: The text prompt used to guide video generation.
*   `BASE64_ENCODED_IMAGE`: A base64-bytes encoded style image.
*   `IMAGE_MIME_TYPE`: The MIME type of the input image. Only one of the following:
    *   `image/jpeg`
    *   `image/png`
*   `OUTPUT_STORAGE_URI`: Optional: The Cloud Storage bucket to store the output videos. If not provided, video bytes are returned in the response. For example: `gs://video-bucket/output/`.
*   `RESPONSE_COUNT`: The number of video files you want to generate. Accepted integer values: 1-4.
*   **Additional optional parameters**
    
    Use the following optional variables depending on your use case. Add some or all of the following parameters in the `"parameters": {}` object.
    
    ```
"parameters": {
  "aspectRatio": "ASPECT_RATIO",
  "negativePrompt": "NEGATIVE_PROMPT",
  "personGeneration": "PERSON_SAFETY_SETTING",
  // "resolution": RESOLUTION, // Veo 3 models only
  "sampleCount": RESPONSE_COUNT,
  "seed": SEED_NUMBER
}
```

    
    *   `ASPECT_RATIO`: Optional: A string value that describes the aspect ratio of the generated videos. You can use the following values:
        
        *   `"16:9"` for landscape
        *   `"9:16"` for portrait
        
        The default value is `"16:9"`
        
    *   `NEGATIVE_PROMPT`: Optional: A string value that describes content that you want to prevent the model from generating.
    *   `PERSON_SAFETY_SETTING`: Optional: A string value that controls the safety setting for generating people or face generation. You can use the following values:
        
        *   `"allow_adult"`: Only allow generation of adult people and faces.
        *   `"disallow"`: Doesn't generate people or faces.
        
        The default value is `"allow_adult"`.
        
    *   `RESOLUTION`: Optional: A string value that controls the resolution of the generated video. Supported by **Veo 3 models only.** You can use the following values:
        
        *   `"720p"`
        *   `"1080p"`
        *   `"4k"` (Veo 3.1 Preview models only)
        
        The default value is `"720p"`.
        
    *   `RESPONSE_COUNT`: Optional. An integer value that describes the number of videos to generate. The accepted range of values is `1`\-`4`.
    *   `SEED_NUMBER`: Optional. An uint32 value that the model uses to generate deterministic videos. Specifying a seed number with your request without changing other parameters guides the model to produce the same videos. The accepted range of values is `0`\-`4294967295`.
    

HTTP method and URL:

```
POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning
```


Request JSON body:

```
{
  "instances": [
    {
      "prompt": "TEXT_PROMPT",
      "referenceImages": [
        {
          "image": {
            "bytesBase64Encoded": "BASE64_ENCODED_IMAGE",
            "mimeType": "IMAGE_MIME_TYPE"
          },
          "referenceType": "style"
        }
      ]
    }
  ],
  "parameters": {
    "durationSeconds": 8,
    "storageUri": "OUTPUT_STORAGE_URI",
    "sampleCount": RESPONSE_COUNT
  }
}

```


To send your request, choose one of these options:

#### curl

Save the request body in a file named `request.json`, and execute the following command:

```
curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning"
```


#### PowerShell

Save the request body in a file named `request.json`, and execute the following command:

```
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
    -Method POST `
    -Headers $headers `
    -ContentType: "application/json; charset=utf-8" `
    -InFile request.json `
    -Uri "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning" | Select-Object -Expand Content
```


This request returns a full operation name with a unique operation ID. Use this full operation name to poll that status of the video generation request.

```
{
  "name":
  "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/a1b07c8e-7b5a-4aba-bb34-3e1ccb8afcc8"
}

```


### Poll the status of the video generation long-running operation

Check the status of the video generation long-running operation.

### REST

Before using any of the request data, make the following replacements:

*   PROJECT\_ID: Your Google Cloud [project ID](about:/resource-manager/docs/creating-managing-projects#identifiers).
*   MODEL\_ID: The model ID to use.
*   OPERATION\_ID: The unique operation ID returned in the original generate video request.

HTTP method and URL:

```
POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:fetchPredictOperation
```


Request JSON body:

```
{
  "operationName": "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/OPERATION_ID"
}

```


To send your request, choose one of these options:

#### curl

Save the request body in a file named `request.json`, and execute the following command:

```
curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:fetchPredictOperation"
```


#### PowerShell

Save the request body in a file named `request.json`, and execute the following command:

```
$cred = gcloud auth print-access-token
$headers = @{ "Authorization" = "Bearer $cred" }
Invoke-WebRequest `
    -Method POST `
    -Headers $headers `
    -ContentType: "application/json; charset=utf-8" `
    -InFile request.json `
    -Uri "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:fetchPredictOperation" | Select-Object -Expand Content
```


This request returns information about the operation, including if the operation is still running or is done.

#### Response

```
{
  "name": "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/OPERATION_ID",
  "done": true,
  "response": {
    "raiMediaFilteredCount": 0,
    "@type": "type.googleapis.com/cloud.ai.large_models.vision.GenerateVideoResponse",
    "videos": [
      {
        "gcsUri":"gs://BUCKET_NAME/TIMESTAMPED_FOLDER/sample_0.mp4",
        "mimeType": "video/mp4"
      }
    ]
  }
}

```


Response body (generate video request)
--------------------------------------

Sending a text-to-video or image-to-video request returns the following response:

```
{
  "name": string
}

```




* Response element: name
  * Description: The full operation name of the long-running operation that begins after a video generation request is sent.


### Sample response (generate video request)

```
{
  "name": "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/OPERATION_ID"
}

```


Response body (poll long-running operation)
-------------------------------------------

Polling the status of the original video generation long-running operation returns a response similar to the following:

```
{
   "name": string,
   "done": boolean,
   "response":{
      "@type":"type.googleapis.com/cloud.ai.large_models.vision.GenerateVideoResponse",
      "raiMediaFilteredCount": integer,
      "videos":[
         {
           "gcsUri": string,
           "mimeType": string
         },
         {
           "gcsUri": string,
           "mimeType": string
         },
         {
           "gcsUri": string,
           "mimeType": string
         },
         {
           "gcsUri": string,
           "mimeType": string
         },
      ]
   }
}

```




* Response element: bytesBase64Encoded
  * Description:         A Base64 bytes encoded string that represents the video object.      
* Response element: done
  * Description:         A boolean value that indicates whether the operation is complete.      
* Response element: encoding
  * Description:         The video encoding type.      
* Response element: gcsUri
  * Description:         The Cloud Storage URI of the generated video.      
* Response element: name
  * Description:         The full operation name of the long-running operation that begins after        a video generation request is sent.      
* Response element: raiMediaFilteredCount
  * Description:         Returns a count of videos that Veo filtered due to        responsible AI policies. If no videos are filtered, the returned count is        0.      
* Response element: raiMediaFilteredReasons
  * Description:         Lists the reasons for any Veo filtered videos due to        responsible AI policies. For more information, see         Safety filter code categories.      
* Response element: response
  * Description:         The response body of the long-running operation.      
* Response element: video
  * Description:         The generated video.      


### Sample response (poll long-running operation)

```
{
   "name": "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/OPERATION_ID",
   "done":true,
   "response":{
      "@type":"type.googleapis.com/cloud.ai.large_models.vision.GenerateVideoResponse",
      "raiMediaFilteredCount": 0,
      "videos":[
        {
          "gcsUri":"gs://STORAGE_BUCKET/TIMESTAMPED_SUBDIRECTORY/sample_0.mp4",
          "mimeType":"video/mp4"
        },
        {
          "gcsUri":"gs://STORAGE_BUCKET/TIMESTAMPED_SUBDIRECTORY/sample_1.mp4",
          "mimeType":"video/mp4"
        },
        {
          "gcsUri":"gs://STORAGE_BUCKET/TIMESTAMPED_SUBDIRECTORY/sample_2.mp4",
          "mimeType":"video/mp4"
        },
        {
          "gcsUri":"gs://STORAGE_BUCKET/TIMESTAMPED_SUBDIRECTORY/sample_3.mp4",
          "mimeType":"video/mp4"
        }
      ]
   }
}

```


More information
----------------

*   For more information about using Veo on Vertex AI, see [Generate videos using text and image prompts using Veo](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/video/generate-videos).

What's next
-----------

*   Read Google DeepMind's information on the [Veo model](https://deepmind.google/technologies/veo/).
*   Read the blog post ["Veo and Imagen 3: Announcing new video and image generation models on Vertex AI"](https://cloud.google.com/blog/products/ai-machine-learning/introducing-veo-and-imagen-3-on-vertex-ai).
*   Read the blog post ["New generative media models and tools, built with and for creators"](https://blog.google/technology/ai/google-generative-ai-veo-imagen-3/).

Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.

Last updated 2026-02-05 UTC.